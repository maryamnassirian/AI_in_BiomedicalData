{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMjV5HUJbG8SHBksmdvGgok",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/maryamnassirian/AI_in_BiomedicalData/blob/main/Copy_of_Untitled0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import scipy. io\n",
        "import numpy as np\n",
        "import torch\n",
        "fr_samp= 200\n",
        "path= r'./data/'\n",
        "file_list= os.listdir(path)\n",
        "data= torch.tensor([])\n",
        "mark= torch.tensor([])\n",
        "data_list= []\n",
        "marker_list= []\n",
        "for file_name in file_list:\n",
        "  if file_name.endswith(' .mat'):\n",
        "    file_path= os.path.join(path, file_name)\n",
        "    mat_data= scipy.io.loadmat(file_path)\n",
        "    data_array= np.array(mat_data['o']['data'][0, 0])\n",
        "    marker_array= np.array(mat_data['o']['marker'][0, 0])\n",
        "    torch_data= torch.from_numpy(data_array)\n",
        "    torch_marker= torch.from_numpy(marker_array)\n",
        "    data= torch.cat((data, torch_data), dim=0)\n",
        "    mark= torch.cat((mark, torch_marker), dim=0)\n",
        "mark= mark[:,0]\n",
        "print(mark.shape)\n",
        "print(data.shape)\n"
      ],
      "metadata": {
        "id": "4-qbK1b4OaeP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pr= 0\n",
        "cl_l_ind= []\n",
        "cl_2_ind= []\n",
        "cl_3_ind= []\n",
        "cl_4_ind= []\n",
        "cl_5_ind= []\n",
        "\n",
        "for i in range(mark.shape[0]):\n",
        "  nx= mark[i]\n",
        "  if pr != nx:\n",
        "    if nx== 1:\n",
        "      cl_l_ind.append(i)\n",
        "    elif nx== 2:\n",
        "      cl_2_ind.append(i)\n",
        "    elif nx== 4:\n",
        "      cl_3_ind.append(i)\n",
        "    elif nx== 5:\n",
        "      cl_4_ind.append(i)\n",
        "    elif nx== 6:\n",
        "      cl_5_ind.append(i)\n",
        "  pr= nx\n",
        "print(len(cl_l_ind), len(cl_2_ind), len(cl_3_ind), len(cl_4_ind), len(cl_�"
      ],
      "metadata": {
        "id": "uqZnpIxRba3X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "separation of trials"
      ],
      "metadata": {
        "id": "v04HmkYYdO9H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def trial_sepration(data, mark_indx, length= fr_samp):\n",
        "  output= torch.tensor([])\n",
        "  for index in mark_indx:\n",
        "    output= torch.cat((output, data[index: index+length, :].unsqueeze(\n",
        "  return output"
      ],
      "metadata": {
        "id": "dj_olfrmdLPn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def data_mark_maker(cl_ind, new_mark, dataset= data):\n",
        "    cls_data= trial_sepration(dataset, cl_ind)\n",
        "    cls_mark= torch.full((cls_data.shape), new_mark, dtype=torch.long)[:,�\n",
        "    return cls_data, cls_mark"
      ],
      "metadata": {
        "id": "a7GKqQvzd2XW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "els_l_data, cls_l_mark= data_mark_maker(cl_l_ind, 0)\n",
        "cls_2_data, els_2_mark= data_mark_maker(cl_2_ind, 1)\n",
        "els_3_data, els_3_mark= data_mark_maker(cl_3_ind, 2)\n",
        "cls_4_data, cls_4_mark= data_mark_maker(cl_4_ind, 3)\n",
        "els_5_data, els_5_mark= data_mark_maker(cl_5_ind, 4)"
      ],
      "metadata": {
        "id": "mAzhDucZfVE7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "del cl_l_ind, cl_2_ind, cl_3_ind, cl_4_ind, cl_5_ind"
      ],
      "metadata": {
        "id": "idIEN7lzft39"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "concat cls"
      ],
      "metadata": {
        "id": "FB2mHy06f47b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data= torch.cat((cls_l_data, cls_2_data, cls_3_data, cls_4_data, els_5_data\n",
        "mark= torch.cat((cls_l_mark, cls_2_mark, cls_3_mark, cls_4_mark, els_5_mark"
      ],
      "metadata": {
        "id": "JUbQt67bf20M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "del cls_l_data, cls_2_data, cls_4_data, cls_5_data, cls_3_data\n",
        "del cls_l_mark, cls_2_mark, cls_4_mark, cls_5_mark, cls_3_mark"
      ],
      "metadata": {
        "id": "M5Ku8Voxjjpl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_sample= mark.shape[0]\n",
        "rnd_perm= torch.randperm(num_sample)\n",
        "\n",
        "data= data[rnd_perm].float()\n",
        "mark= mark[rnd_perm].long()"
      ],
      "metadata": {
        "id": "XqImXoXQjvjO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "save data"
      ],
      "metadata": {
        "id": "Qj04feSWj0ol"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data= data.unsqueeze(1).permute(0,1,3,2)\n",
        "torch.save(data, 'data. pth' )\n",
        "torch.save(mark, 'mark. pth')\n",
        "\n",
        "print(data.shape)\n",
        "print(mark.shape)"
      ],
      "metadata": {
        "id": "lHoJW-8-j5BC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "split"
      ],
      "metadata": {
        "id": "_xBKp0SnkRWk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "train ratio= 0.8\n",
        "\n",
        "x_train, x_test, y_train, y_test= train_test_split(data, mark, train_size=\n",
        "x_train, x_valid, y_train, y_valid= train_test_split(x_train, y_train, trc\n",
        "print( 'train: ',  x_train.shape, y_train.shape)\n",
        "print( 'valid: ',  x_valid.shape, y_valid.shape)\n",
        "print('test: ',  x_test.shape, y_test.shape)"
      ],
      "metadata": {
        "id": "IN2af76gkUQR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "data loader"
      ],
      "metadata": {
        "id": "ET9lfLcTyvzz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "train_batch_size= 330\n",
        "valid_batch_size= 330\n",
        "\n",
        "train_dataset= TensorDataset(x_train, y_train)\n",
        "valid_dataset= TensorDataset(x_valid, y_valid)\n",
        "test_dataset= TensorDataset(x_test, y_test)\n",
        "\n",
        "train_loader= DataLoader(train_dataset, batch_size= train_batch_size, shu)\n",
        "valid_loader= DataLoader(valid_dataset, batch_size= valid_batch_size, shu)\n",
        "test_loader= DataLoader(test_dataset, batch_size= valid_batch_size, shuf)\n",
        "\n",
        "print(\"train batch size:\",train_loader.batch_size, \",num of batch:\", len)\n",
        "print(\"valid batch size:\",valid_loader.batch_size, \",num of batch:\", len)\n",
        "print(\"test batch size:\",test_loader.batch_size, \",num of batch:\", len)\n",
        "\n"
      ],
      "metadata": {
        "id": "j8jLeaF3yyII"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x,y= next(iter(train_loader))\n",
        "print(x.shape, y.shape)"
      ],
      "metadata": {
        "id": "D0RhyBFy3kzE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "model"
      ],
      "metadata": {
        "id": "dozSfLx632p_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "hyperparameters"
      ],
      "metadata": {
        "id": "-WfALbZB39jY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num class= 5\n",
        "num_input= 1\n",
        "channel= 22\n",
        "signal_length= 200\n",
        "fs= 200\n",
        "\n",
        "Fl= 8\n",
        "D= 3\n",
        "F2= D*F1\n",
        "kernel_size_1= (1,round(fs/2))\n",
        "kernel_size_2= (channel, 1)\n",
        "kernel_size_3= (1, round(fs/8))\n",
        "kernel_size_4= (1, 1)\n",
        "\n",
        "kernel_avgpool_1= (1,4)\n",
        "kernel_avgpool_2= (1,8)\n",
        "dropout_rate= 0.2\n",
        "ks0= int(round((kernel_size_1[0]-1)/2))\n",
        "ks1= int(round((kernel_size_1[1]-1)/2))\n",
        "kernel_padding_1= (ks0, ks1-1)\n",
        "ks0= int(round((kernel_size_3[0]-1)/2))\n",
        "ks1= int(round((kernel_size_3[1]-1)/2))\n",
        "kernel_padding_3= (ks0, ks1)"
      ],
      "metadata": {
        "id": "eXVFWiXY35qu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "defining the base class"
      ],
      "metadata": {
        "id": "H8BS1PQ78GP-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "class EEGNet(nn.Module):\n",
        "def __init__(self):\n",
        "super().__init__()\n",
        "# Layer 1\n",
        "self.conv2d= nn.Conv2d(num_input, F1, kernel_size_1, padding=kerne\n",
        "self.Batch_normalization_1= nn.BatchNorm2d(Fl)\n",
        "# Layer 2\n",
        "self.Depthwise_conv2D= nn.Conv2d(Fl, D*F1, kernel_size_2, groups=\n",
        "self.Batch_normalization_2= nn.BatchNorm2d(D*Fl)\n",
        "self.Elu= nn.ELU()\n",
        "self.Average_pooling2D_1= nn.AvgPool2d(kernel_avgpool_1)\n",
        "self.Dropout= nn.Dropout2d(dropout_rate)\n",
        "# Layer 3\n",
        "self.Separable_conv2D_depth= nn.Conv2d(D*F1, D*F1, kernel_size_3,\n",
        "padding= kernel_padding_3,\n",
        "self.Separable_conv2D_point= nn.Conv2d(D*F1, F2, kernel_size_4)\n",
        "self.Batch_normalization_3= nn.BatchNorm2d(F2)\n",
        "self.Average_pooling2D_2= nn.AvgPool2d(kernel_avgpool_2)\n",
        "# Layer 4\n",
        "self.Flatten= nn.Flatten()\n",
        "self.Dense= nn.Linear(F2* round(signal_length/32), num_class)\n",
        "self.Softmax= nn.Softmax(dim= 1)\n",
        "\n",
        "\n",
        "def forward(self, x):\n",
        "    # Layer 1\n",
        "    y= self.Batch_normalization_1(self.conv2d(x))\n",
        "    # Layer 2\n",
        "    y= self.Batch_normalization_2(self.Depthwise_conv2D(y))\n",
        "    v= self. Elu (y)\n",
        "    y= self.Dropout(self.Average_pooling2D_1(y))\n",
        "    # Layer 3\n",
        "    y= self.Separable_conv2D_depth(y)\n",
        "    y= self.Batch_normalization_3(self.Separable_conv2D_point(y))\n",
        "    v= self. Elu (y)\n",
        "    y= self.Dropout(self.Average_pooling2D_2(y))\n",
        "    # Layer 4\n",
        "    y= self.Flatten(y)\n",
        "    y= self.Dense(y)\n",
        "    y= self.Softmax(y)\n",
        "\n",
        "    return y"
      ],
      "metadata": {
        "id": "EfZ5qNl18Rk-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "model= EEGNet()\n",
        "eta= 0.01\n",
        "loss_fn= nn.CrossEntropyloss()\n",
        "optimizer= optim.NAdam(model.parameters(), lr= eta)"
      ],
      "metadata": {
        "id": "WjsXYSk9-SlY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "train"
      ],
      "metadata": {
        "id": "-dqqL3AQ-lFg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "one epoch"
      ],
      "metadata": {
        "id": "agXx-POw-xqi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchmetrics import Accuracy\n",
        "\n",
        "def train_one_epoch(model, train_loader, loss_fn, optimizer):\n",
        "    model.train()\n",
        "    loss_train= AverageMeter()\n",
        "    acc_train= Accuracy(task= \"multiclass\", num_classes= num_class)\n",
        "\n",
        "    for i, (inputs, targets) in enumerate(train_loader):\n",
        "        outputs= model(inputs)\n",
        "        loss= loss_fn(outputs, targets)\n",
        "\n",
        "        loss.backward()\n",
        "        nn.utils.clip_grad_norm_(model.parameters(), 1)\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "        loss_train.update(loss.item())\n",
        "        acc_train(outputs, targets.int())\n",
        "return model, loss_train.avg, acc_train.compute().item()"
      ],
      "metadata": {
        "id": "AhtcbcTT-vq7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchmetrics import F1Score\n",
        "\n",
        "def test_one_epoch(model, test_loader, loss_fn, optimizer):\n",
        "    model. eval ()\n",
        "    loss_test= AverageMeter()\n",
        "    acc_test= Accuracy(task= \"multiclass\", num_classes= num_class)\n",
        "    f1_test= F1Score(task= \"multiclass\", num_classes= num_class)\n",
        "    for i, (inputs, targets) in enumerate(test_loader):\n",
        "        outputs= model(inputs)\n",
        "        loss= loss_fn(outputs, targets)\n",
        "        loss_test.update(loss.item())\n",
        "        acc_test(outputs, targets.int())\n",
        "        f1_test(outputs, targets.int())\n",
        "return model, loss_test.avg, acc_test.compute().item(), f1_test.compu1"
      ],
      "metadata": {
        "id": "Z9BlSsKS_sBU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "util"
      ],
      "metadata": {
        "id": "d2S002IYA0tE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class AverageMeter(object):\n",
        "\"\"\"Computes and stores the average and current value\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.reset()\n",
        "    def reset(self):\n",
        "        self.val = 0\n",
        "        self.avg= 0\n",
        "        self.sum= 0\n",
        "        self.count = 0\n",
        "\n",
        "def update(self, val, n=1):\n",
        "        self.val= val\n",
        "        self.sum+= val* n\n",
        "        self.count+= n\n",
        "        self.avg= self.sum / self.count"
      ],
      "metadata": {
        "id": "b64g8LNCA3DJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "training"
      ],
      "metadata": {
        "id": "KK2_mCyzB3yp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs= 50\n",
        "loss_train_hist, acc_train_hist= [], []\n",
        "loss_valid_hist, acc_valid_hist= [], []\n",
        "loss_test_hist, acc_test_hist= [], []\n",
        "\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model, loss_train, acc_train= train_one_epoch(model, train_loader, lo�\n",
        "    model, loss_valid, acc_valid, _= test_one_epoch(model, valid_loader, J\n",
        "    model, loss_test, acc_test, flscore= test_one_epoch(model, test_loader\n",
        "\n",
        "    loss_train_hist.append(loss_train)\n",
        "    acc_train_hist.append(acc_train)\n",
        "    loss_valid_hist.append(loss_valid)\n",
        "    acc_valid_hist.append(acc_valid)\n",
        "    loss_test_hist.append(loss_test)\n",
        "    acc_test_hist.append(acc_test)\n",
        "    if (epoch%5== 0):\n",
        "print(f'epoch {epoch}:')\n",
        "print(f' Loss= {loss_train:.4}, Accuracy= {int(acc_train*100)}%')\n",
        "print(f' Loss= {loss_valid:.4}, Accuracy= {int(acc_valid*100)}%')\n",
        "print(f' Loss= {loss_test:.4}, Accuracy= {int(acc_test*100)}%')\n",
        "print(f' f1 score= {(f1score*100):.4}\\n')"
      ],
      "metadata": {
        "id": "tGb5GXGLB6at"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "learning curve\n",
        "loss"
      ],
      "metadata": {
        "id": "wI1AV95CDGu8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(range(num_epochs), loss_train_hist, 'b-', label= 'Train')\n",
        "plt.plot(range(num_epochs), loss_valid_hist, 'k-', label= 'Valid')\n",
        "plt.plot(range(num_epochs), loss_test_hist, 'g-', label= 'Test')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.grid(True)\n",
        "plt.legend()"
      ],
      "metadata": {
        "id": "__R7QGcFDMBL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "accuracy"
      ],
      "metadata": {
        "id": "C0MUgkjwDmXk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(range(num_epochs), acc_train_hist, 'b-', label= 'Train')\n",
        "plt.plot(range(num_epochs), acc_valid_hist, 'k-', label= 'Valid')\n",
        "plt.plot(range(num_epochs), acc_test_hist, 'g-', label= 'Test')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Acc')\n",
        "plt.grid(True)\n",
        "plt.legend()"
      ],
      "metadata": {
        "id": "Rt8b43IXDoJR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}